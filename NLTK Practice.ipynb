{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization and Text Preprocessing with NLTK and Python\n",
    "Tokenization plays a crucial role in Natural Language Processing (NLP) as it breaks down text into smaller units called tokens, which can be words, sentences, or even characters. Tokenization serves as the foundation for various NLP tasks such as text classification, sentiment analysis, and named entity recognition. NLTK offers powerful tokenization capabilities that facilitate efficient processing of textual data.\n",
    "\n",
    "NLTK’s word tokenization allows you to split text into individual words or tokens. This process is essential for analyzing the linguistic structure of a sentence and extracting meaningful information from it. NLTK provides different tokenization methods, including the default word_tokenize() function and alternative options like TreebankWordTokenizer and RegexpTokenizer.\n",
    "\n",
    "NLTK also provides sentence tokenization, which is the process of splitting a document or paragraph into individual sentences. Sentence tokenization helps in tasks like document summarization or machine translation. NLTK’s sent_tokenize() function efficiently handles this task by considering various sentence boundary rules and exceptions.\n",
    "\n",
    "After tokenization, it is often necessary to preprocess the text further to enhance its quality and remove noise. NLTK offers several preprocessing techniques to assist in this process:\n",
    "\n",
    "Removing Stop Words:\n",
    "Stop words are common words like “and,” “the,” or “is” that do not contribute much to the meaning of a sentence. They can be safely removed during preprocessing to reduce data size and improve computational efficiency without losing significant information. NLTK provides a predefined list of stop words in multiple languages that can be easily used with its stopwords module.\n",
    "Stemming:\n",
    "Stemming reduces words to their base or root form by removing suffixes using simple heuristics. For example, it converts words like “running,” “runs,” and “ran” into their common stem “run.” NLTK includes several stemming algorithms such as PorterStemmer, LancasterStemmer, or SnowballStemmer that you can utilize for different purposes.\n",
    "Lemmatization:\n",
    "While stemming provides an approximate root form, lemmatization aims to obtain the actual base form of a word known as the lemma. Unlike stemming, lemmatization considers the structure and meaning of words, which makes it more accurate but computationally expensive. NLTK provides the WordNetLemmatizer for English lemmatization.\n",
    "Handling Special Characters:\n",
    "Text data often contains special characters like punctuation marks, hashtags, URLs, or emoticons, which may not be necessary in some NLP tasks. NLTK offers various methods to handle these characters effectively. For instance, removing or replacing special characters using regular expressions can ensure smoother processing and prevent unnecessary noise in your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Meddieek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Meddieek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Meddieek\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens:\n",
      "['Tokenization', 'is', 'an', 'important', 'step', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', '.', 'It', 'breaks', 'down', 'text', 'into', 'smaller', 'units', 'called', 'tokens', '.', 'These', 'tokens', 'can', 'be', 'words', ',', 'sentences', ',', 'or', 'even', 'characters', '.']\n",
      "\n",
      "Sentences:\n",
      "['Tokenization is an important step in Natural Language Processing (NLP).', 'It breaks down text into smaller units called tokens.', 'These tokens can be words, sentences, or even characters.']\n",
      "\n",
      "Tokens after removing stop words:\n",
      "['Tokenization', 'important', 'step', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', '.', 'breaks', 'text', 'smaller', 'units', 'called', 'tokens', '.', 'tokens', 'words', ',', 'sentences', ',', 'even', 'characters', '.']\n",
      "\n",
      "Stemmed Tokens:\n",
      "['token', 'import', 'step', 'natur', 'languag', 'process', '(', 'nlp', ')', '.', 'break', 'text', 'smaller', 'unit', 'call', 'token', '.', 'token', 'word', ',', 'sentenc', ',', 'even', 'charact', '.']\n",
      "\n",
      "Lemmatized Tokens:\n",
      "['Tokenization', 'important', 'step', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', '.', 'break', 'text', 'smaller', 'unit', 'called', 'token', '.', 'token', 'word', ',', 'sentence', ',', 'even', 'character', '.']\n",
      "\n",
      "Tokens after handling special characters:\n",
      "['Tokenization', 'is', 'an', 'important', 'step', 'in', 'Natural', 'Language', 'Processing', 'NLP', 'It', 'breaks', 'down', 'text', 'into', 'smaller', 'units', 'called', 'tokens', 'These', 'tokens', 'can', 'be', 'words', 'sentences', 'or', 'even', 'characters']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK resources (only required once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample text for demonstration\n",
    "text = \"Tokenization is an important step in Natural Language Processing (NLP). It breaks down text into smaller units called tokens. These tokens can be words, sentences, or even characters.\"\n",
    "\n",
    "# Tokenization - Word Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Word Tokens:\")\n",
    "print(tokens)\n",
    "print()\n",
    "\n",
    "# Tokenization - Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentences:\")\n",
    "print(sentences)\n",
    "print()\n",
    "\n",
    "# Text Preprocessing - Removing Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.casefold() not in stop_words]\n",
    "print(\"Tokens after removing stop words:\")\n",
    "print(filtered_tokens)\n",
    "print()\n",
    "\n",
    "# Text Preprocessing - Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(\"Stemmed Tokens:\")\n",
    "print(stemmed_tokens)\n",
    "print()\n",
    "\n",
    "# Text Preprocessing - Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "print(\"Lemmatized Tokens:\")\n",
    "print(lemmatized_tokens)\n",
    "print()\n",
    "\n",
    "# Text Preprocessing - Handling Special Characters\n",
    "special_chars = set(string.punctuation)\n",
    "filtered_tokens = [token for token in tokens if token not in special_chars]\n",
    "print(\"Tokens after handling special characters:\")\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part-of-Speech Tagging\n",
    "Part-of-speech (POS) tagging is a vital process in NLP that involves assigning tags to words in a text, indicating their grammatical category or function within a sentence. POS tagging aids in understanding the structure and meaning of a sentence, which is crucial for various NLP applications such as text analysis, information retrieval, machine translation, and sentiment analysis.\n",
    "\n",
    "NLTK provides several methods and taggers for performing POS tagging. One of the popular taggers in NLTK is the pos_tag() function, which uses the Penn Treebank tagset. Here’s a detailed Python code example that demonstrates POS tagging using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Meddieek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK: NNP\n",
      "provides: VBZ\n",
      "powerful: JJ\n",
      "tools: NNS\n",
      "for: IN\n",
      "performing: VBG\n",
      "POS: NNP\n",
      "tagging: NN\n",
      ".: .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK resources (only required once)\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample text for demonstration\n",
    "text = \"NLTK provides powerful tools for performing POS tagging.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Print the POS tags\n",
    "for token, pos_tag in pos_tags:\n",
    "    print(f\"{token}: {pos_tag}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment Analysis with NLTK\n",
    "Sentiment analysis, also known as opinion mining, is a crucial area of NLP that involves determining the sentiment expressed in a piece of text. It has various applications, including social media monitoring, brand reputation management, market research, and customer feedback analysis. NLTK provides powerful tools and techniques to perform sentiment analysis efficiently. NLTK supports multiple approaches for sentiment analysis, including rule-based and machine learning methods. Rule-based approaches rely on predefined sets of linguistic rules or lexicons to determine the sentiment of words or phrases in a text.\n",
    "\n",
    "One popular rule-based approach is the Vader sentiment analysis tool included in NLTK, which provides a pre-trained model for analyzing sentiment. Machine learning methods leverage labeled datasets to train models that can automatically classify text into positive, negative, or neutral sentiments. NLTK offers functionality to preprocess and prepare data for machine learning classification models. It also provides access to various classifiers like Naive Bayes, Maximum Entropy, and Support Vector Machines for sentiment analysis tasks.\n",
    "\n",
    "To illustrate an end-to-end example of sentiment analysis using NLTK’s built-in functionalities, let’s consider a scenario where we want to analyze the sentiments expressed in a collection of Twitter tweets about a particular product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Meddieek\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00       0.0\n",
      "    positive       0.00      0.00      0.00       1.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n",
      "Tweet: This product exceeded my expectations!\n",
      "Sentiment Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Tweet: I'm really disappointed with the customer service.\n",
      "Sentiment Scores: {'neg': 0.361, 'neu': 0.639, 'pos': 0.0, 'compound': -0.5256}\n",
      "\n",
      "Tweet: The price seems fair for the quality.\n",
      "Sentiment Scores: {'neg': 0.0, 'neu': 0.723, 'pos': 0.277, 'compound': 0.3182}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Meddieek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Meddieek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "d:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Download necessary NLTK resources (only required once)\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load labeled data for training a sentiment classifier\n",
    "# Assumes the data is in the format: tweet,label (e.g., \"I love this product,positive\")\n",
    "labeled_data = [\n",
    "    (\"I love this product\", \"positive\"),\n",
    "    (\"This product is terrible\", \"negative\"),\n",
    "    (\"The quality could be better\", \"neutral\"),\n",
    "    # Add more labeled data here...\n",
    "]\n",
    "\n",
    "# Preprocess the labeled data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "preprocessed_data = []\n",
    "labels = []\n",
    "\n",
    "for tweet, label in labeled_data:\n",
    "    tokens = word_tokenize(tweet.lower())\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    preprocessed_tweet = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    preprocessed_data.append(preprocessed_tweet)\n",
    "    labels.append(label)\n",
    "\n",
    "# Split the preprocessed data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the preprocessed data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Support Vector Machine (SVM) classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Evaluate the trained classifier on the testing set\n",
    "y_pred = svm_classifier.predict(X_test_vectors)\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report)\n",
    "\n",
    "# Sentiment analysis of new, unseen tweets using Vader sentiment analyzer\n",
    "unseen_tweets = [\n",
    "    \"This product exceeded my expectations!\",\n",
    "    \"I'm really disappointed with the customer service.\",\n",
    "    \"The price seems fair for the quality.\",\n",
    "    # Add more unseen tweets here...\n",
    "]\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "for tweet in unseen_tweets:\n",
    "    sentiment_scores = analyzer.polarity_scores(tweet)\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    print(f\"Sentiment Scores: {sentiment_scores}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Named Entity Recognition with NLTK\n",
    "Named entity recognition (NER) is a natural language processing (NLP) task that identifies and classifies named entities in text into predefined categories, such as people, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. NER is a crucial step in information extraction, which is the process of automatically extracting structured information from unstructured text data.\n",
    "\n",
    "An example:\n",
    "\n",
    "Step 1: Download necessary NLTK resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Meddieek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Meddieek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources (only required once)\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Prepare text for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text for demonstration\n",
    "text = \"Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne. Its headquarters are located in Cupertino, California.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Perform NER using NLTK’s pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Apple | Type: PERSON\n",
      "Entity: Inc. | Type: ORGANIZATION\n",
      "Entity: Steve Jobs | Type: PERSON\n",
      "Entity: Steve Wozniak | Type: PERSON\n",
      "Entity: Ronald Wayne | Type: PERSON\n",
      "Entity: Cupertino | Type: GPE\n",
      "Entity: California | Type: GPE\n"
     ]
    }
   ],
   "source": [
    "# Apply NER using NLTK's pre-trained models\n",
    "ner_tags = ne_chunk(nltk.pos_tag(tokens))\n",
    "\n",
    "# Print the named entities\n",
    "for chunk in ner_tags:\n",
    "    if hasattr(chunk, 'label'):\n",
    "        print(f\"Entity: {' '.join(c[0] for c in chunk)} | Type: {chunk.label()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
